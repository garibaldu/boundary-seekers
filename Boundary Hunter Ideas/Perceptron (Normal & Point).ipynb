{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from autograd import grad\n",
    "\n",
    "def generateChevronData():\n",
    "    xBounds = [-50, 50]\n",
    "    yBounds = [-50, 50]\n",
    "    totalPoints = 100\n",
    "    \n",
    "    points = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(0, totalPoints):\n",
    "        x = random.randint(xBounds[0], xBounds[1])\n",
    "        y = random.randint(yBounds[0], yBounds[1])\n",
    "        \n",
    "        if x >= y and x <= -y:\n",
    "            points.append([1, x/50.0,y/50.0])\n",
    "            targets.append(0)\n",
    "        else:\n",
    "            points.append([1, x/50.0,y/50.0])\n",
    "            targets.append(1)\n",
    "        \n",
    "    return np.array(points), np.array(targets)\n",
    "    \n",
    "def plotScatter(points):\n",
    "    xs = [x[1] for x in points]\n",
    "    ys = [y[2] for y in points]\n",
    "    \n",
    "    plt.scatter(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(phi):\n",
    "    return 1.0/(1.0 + np.exp(-phi))\n",
    "\n",
    "def MSE(weights):\n",
    "    predictions = logisticPrediction(weights, points)\n",
    "    return 1.0/2.0 * np.sum(np.power((targets - predictions), 2))\n",
    "\n",
    "def logisticPrediction(weights, p):\n",
    "    return np.array(list(map(lambda x: predict(weights, x), p))) \n",
    "    \n",
    "def predict(weights, inputs):\n",
    "    n = np.array([weights[0], weights[1], weights[2]])\n",
    "    i = np.array([1, weights[3] - inputs[1], weights[4] - inputs[2]])\n",
    "#     n = np.array([weights[0], weights[1] - weights[3], weights[2] - weights[4]])\n",
    "    return sigmoid(np.dot(n, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeGradient(weights, example, target):\n",
    "    prediction = predict(weights, example)\n",
    "    dE_dO = computeErrorDifferential(prediction, target)\n",
    "    \n",
    "    dO_dZ = prediction * (1-prediction)\n",
    "    \n",
    "    dZ_d0 = example[0]\n",
    "    dZ_d1 = (weights[3] - example[1])\n",
    "    dZ_d2 = (weights[4] - example[2])\n",
    "    dZ_d3 = weights[1]\n",
    "    dZ_d4 = weights[2]\n",
    "    \n",
    "    dE_dZ = dE_dO * dO_dZ\n",
    "    \n",
    "    grad = np.zeros(len(weights))#[0.0, 0.0, 0.0]\n",
    "    grad[0] = dZ_d0 * dE_dZ\n",
    "    grad[1] = dZ_d1 * dE_dZ\n",
    "    grad[2] = dZ_d2 * dE_dZ\n",
    "    grad[3] = dZ_d3 * dE_dZ\n",
    "    grad[4] = dZ_d4 * dE_dZ\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def computeErrorDifferential(prediction, target):\n",
    "    return -(target - prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainBoundaryHunter():\n",
    "    weights = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    \n",
    "    print(\"Initial Loss: \", MSE(weights))\n",
    "    for i in range(0, 50000):\n",
    "#         g = trainingGradient(weights) * 0.01\n",
    "        if i % 1000 == 0:\n",
    "            print()\n",
    "            print(\"Loss Before: \" + str(MSE(weights)))\n",
    "\n",
    "        weights = computeStep(weights)\n",
    "#         weights -= g\n",
    "    \n",
    "        if i % 1000 == 0:\n",
    "            print(\"Loss After [i = \" + str(i) + \"]: \" + str(MSE(weights)))\n",
    "            print(weights)\n",
    "            \n",
    "    print(\"Trained Loss: \", MSE(weights))    \n",
    "    print(\"Weights: \", weights)\n",
    "    return weights\n",
    "\n",
    "def computeStep(weights):\n",
    "    totalG = np.zeros(len(weights))\n",
    "    totalE = 0\n",
    "    for i in range(0, len(points)):\n",
    "        g = computeGradient(weights, points[i], targets[i])\n",
    "        totalG += g     \n",
    "        \n",
    "#     totalG = totalG * (1/len(points))\n",
    "    \n",
    "    weights -= totalG * 0.01\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 0:  35\n",
      "Type 1:  65\n",
      "Initial Loss:  12.5\n",
      "\n",
      "Loss Before: 12.5\n",
      "Loss After [i = 0]: 12.1601229227\n",
      "[ 0.0375  -0.00395 -0.04505  0.       0.     ]\n",
      "\n",
      "Loss Before: 4.0105187379\n",
      "Loss After [i = 1000]: 4.01036066808\n",
      "[ 0.5330375  -1.20405261 -4.91453145 -0.0421987  -0.29838986]\n",
      "\n",
      "Loss Before: 3.93645016905\n",
      "Loss After [i = 2000]: 3.93641795726\n",
      "[ 0.53551902 -1.46358658 -5.71125388 -0.04549032 -0.31150962]\n",
      "\n",
      "Loss Before: 3.91737163578\n",
      "Loss After [i = 3000]: 3.91736095275\n",
      "[ 0.53647825 -1.59243363 -6.12266109 -0.0469543  -0.31717893]\n",
      "\n",
      "Loss Before: 3.91042738407\n",
      "Loss After [i = 4000]: 3.91042307703\n",
      "[ 0.53697906 -1.6689914  -6.37235949 -0.04777066 -0.32030655]\n",
      "\n",
      "Loss Before: 3.90748735538\n",
      "Loss After [i = 5000]: 3.90748543061\n",
      "[ 0.53727738 -1.71832073 -6.53531125 -0.04827582 -0.32223153]\n",
      "\n",
      "Loss Before: 3.90613457383\n",
      "Loss After [i = 6000]: 3.9061336591\n",
      "[ 0.53746839 -1.75156557 -6.64603296 -0.0486072  -0.32349036]\n",
      "\n",
      "Loss Before: 3.90547954589\n",
      "Loss After [i = 7000]: 3.9054790937\n",
      "[ 0.53759621 -1.77459664 -6.72316043 -0.04883254 -0.32434474]\n",
      "\n",
      "Loss Before: 3.90515166177\n",
      "Loss After [i = 8000]: 3.90515143225\n",
      "[ 0.53768421 -1.7908409  -6.77776672 -0.04898943 -0.32493878]\n",
      "\n",
      "Loss Before: 3.90498380494\n",
      "Loss After [i = 9000]: 3.90498368632\n",
      "[ 0.53774598 -1.80243821 -6.81685607 -0.0491004  -0.32535862]\n",
      "\n",
      "Loss Before: 3.9048965232\n",
      "Loss After [i = 10000]: 3.9048964611\n",
      "[ 0.53778991 -1.81078786 -6.84505253 -0.04917976 -0.32565871]\n",
      "\n",
      "Loss Before: 3.90485063731\n",
      "Loss After [i = 11000]: 3.90485060451\n",
      "[ 0.53782145 -1.81683507 -6.86550163 -0.04923697 -0.32587491]\n",
      "\n",
      "Loss Before: 3.90482632426\n",
      "Loss After [i = 12000]: 3.90482630682\n",
      "[ 0.53784424 -1.82123331 -6.88038931 -0.04927843 -0.32603157]\n",
      "\n",
      "Loss Before: 3.90481336888\n",
      "Loss After [i = 13000]: 3.90481335957\n",
      "[ 0.53786079 -1.82444198 -6.89125819 -0.0493086  -0.32614554]\n",
      "\n",
      "Loss Before: 3.90480643728\n",
      "Loss After [i = 14000]: 3.90480643228\n",
      "[ 0.53787285 -1.82678798 -6.89920907 -0.04933062 -0.3262287 ]\n",
      "\n",
      "Loss Before: 3.90480271758\n",
      "Loss After [i = 15000]: 3.9048027149\n",
      "[ 0.53788166 -1.82850599 -6.90503384 -0.04934672 -0.32628951]\n",
      "\n",
      "Loss Before: 3.90480071716\n",
      "Loss After [i = 16000]: 3.90480071572\n",
      "[ 0.53788811 -1.82976559 -6.90930559 -0.04935851 -0.32633405]\n",
      "\n",
      "Loss Before: 3.90479963965\n",
      "Loss After [i = 17000]: 3.90479963887\n",
      "[ 0.53789284 -1.83068988 -6.91244084 -0.04936716 -0.3263667 ]\n",
      "\n",
      "Loss Before: 3.90479905858\n",
      "Loss After [i = 18000]: 3.90479905816\n",
      "[ 0.5378963  -1.83136854 -6.91474326 -0.04937351 -0.32639066]\n",
      "\n",
      "Loss Before: 3.90479874496\n",
      "Loss After [i = 19000]: 3.90479874474\n",
      "[ 0.53789885 -1.83186709 -6.91643479 -0.04937817 -0.32640826]\n",
      "\n",
      "Loss Before: 3.90479857559\n",
      "Loss After [i = 20000]: 3.90479857547\n",
      "[ 0.53790072 -1.83223344 -6.91767789 -0.04938159 -0.32642119]\n",
      "\n",
      "Loss Before: 3.90479848408\n",
      "Loss After [i = 21000]: 3.90479848401\n",
      "[ 0.53790209 -1.83250271 -6.91859165 -0.04938411 -0.32643068]\n",
      "\n",
      "Loss Before: 3.90479843462\n",
      "Loss After [i = 22000]: 3.90479843458\n",
      "[ 0.5379031  -1.83270067 -6.91926344 -0.04938596 -0.32643767]\n",
      "\n",
      "Loss Before: 3.90479840787\n",
      "Loss After [i = 23000]: 3.90479840786\n",
      "[ 0.53790384 -1.83284622 -6.91975739 -0.04938732 -0.3264428 ]\n",
      "\n",
      "Loss Before: 3.90479839342\n",
      "Loss After [i = 24000]: 3.9047983934\n",
      "[ 0.53790439 -1.83295324 -6.92012061 -0.04938832 -0.32644657]\n",
      "\n",
      "Loss Before: 3.9047983856\n",
      "Loss After [i = 25000]: 3.90479838559\n",
      "[ 0.53790479 -1.83303195 -6.92038771 -0.04938905 -0.32644935]\n",
      "\n",
      "Loss Before: 3.90479838137\n",
      "Loss After [i = 26000]: 3.90479838136\n",
      "[ 0.53790508 -1.83308983 -6.92058415 -0.04938959 -0.32645139]\n",
      "\n",
      "Loss Before: 3.90479837908\n",
      "Loss After [i = 27000]: 3.90479837908\n",
      "[ 0.5379053  -1.8331324  -6.92072862 -0.04938999 -0.32645289]\n",
      "\n",
      "Loss Before: 3.90479837784\n",
      "Loss After [i = 28000]: 3.90479837784\n",
      "[ 0.53790546 -1.8331637  -6.92083488 -0.04939028 -0.32645399]\n",
      "\n",
      "Loss Before: 3.90479837717\n",
      "Loss After [i = 29000]: 3.90479837717\n",
      "[ 0.53790558 -1.83318673 -6.92091303 -0.0493905  -0.3264548 ]\n",
      "\n",
      "Loss Before: 3.90479837681\n",
      "Loss After [i = 30000]: 3.90479837681\n",
      "[ 0.53790566 -1.83320366 -6.9209705  -0.04939065 -0.3264554 ]\n",
      "\n",
      "Loss Before: 3.90479837661\n",
      "Loss After [i = 31000]: 3.90479837661\n",
      "[ 0.53790573 -1.83321612 -6.92101278 -0.04939077 -0.32645584]\n",
      "\n",
      "Loss Before: 3.90479837651\n",
      "Loss After [i = 32000]: 3.90479837651\n",
      "[ 0.53790577 -1.83322528 -6.92104387 -0.04939086 -0.32645616]\n",
      "\n",
      "Loss Before: 3.90479837645\n",
      "Loss After [i = 33000]: 3.90479837645\n",
      "[ 0.53790581 -1.83323202 -6.92106674 -0.04939092 -0.3264564 ]\n",
      "\n",
      "Loss Before: 3.90479837642\n",
      "Loss After [i = 34000]: 3.90479837642\n",
      "[ 0.53790583 -1.83323698 -6.92108356 -0.04939096 -0.32645658]\n",
      "\n",
      "Loss Before: 3.9047983764\n",
      "Loss After [i = 35000]: 3.9047983764\n",
      "[ 0.53790585 -1.83324062 -6.92109594 -0.049391   -0.3264567 ]\n",
      "\n",
      "Loss Before: 3.90479837639\n",
      "Loss After [i = 36000]: 3.90479837639\n",
      "[ 0.53790586 -1.8332433  -6.92110504 -0.04939102 -0.3264568 ]\n",
      "\n",
      "Loss Before: 3.90479837639\n",
      "Loss After [i = 37000]: 3.90479837639\n",
      "[ 0.53790587 -1.83324528 -6.92111173 -0.04939104 -0.32645687]\n",
      "\n",
      "Loss Before: 3.90479837639\n",
      "Loss After [i = 38000]: 3.90479837639\n",
      "[ 0.53790588 -1.83324673 -6.92111665 -0.04939106 -0.32645692]\n",
      "\n",
      "Loss Before: 3.90479837639\n",
      "Loss After [i = 39000]: 3.90479837639\n",
      "[ 0.53790589 -1.83324779 -6.92112027 -0.04939107 -0.32645696]\n",
      "\n",
      "Loss Before: 3.90479837638\n",
      "Loss After [i = 40000]: 3.90479837638\n",
      "[ 0.53790589 -1.83324858 -6.92112294 -0.04939107 -0.32645699]\n",
      "\n",
      "Loss Before: 3.90479837638\n",
      "Loss After [i = 41000]: 3.90479837638\n",
      "[ 0.53790589 -1.83324915 -6.92112489 -0.04939108 -0.32645701]\n",
      "\n",
      "Loss Before: 3.90479837638\n",
      "Loss After [i = 42000]: 3.90479837638\n",
      "[ 0.5379059  -1.83324958 -6.92112634 -0.04939108 -0.32645702]\n",
      "\n",
      "Loss Before: 3.90479837638\n",
      "Loss After [i = 43000]: 3.90479837638\n",
      "[ 0.5379059  -1.83324989 -6.9211274  -0.04939109 -0.32645703]\n",
      "\n",
      "Loss Before: 3.90479837638\n",
      "Loss After [i = 44000]: 3.90479837638\n",
      "[ 0.5379059  -1.83325012 -6.92112817 -0.04939109 -0.32645704]\n",
      "\n",
      "Loss Before: 3.90479837638\n",
      "Loss After [i = 45000]: 3.90479837638\n",
      "[ 0.5379059  -1.83325029 -6.92112875 -0.04939109 -0.32645705]\n",
      "\n",
      "Loss Before: 3.90479837638\n",
      "Loss After [i = 46000]: 3.90479837638\n",
      "[ 0.5379059  -1.83325041 -6.92112917 -0.04939109 -0.32645705]\n",
      "\n",
      "Loss Before: 3.90479837638\n",
      "Loss After [i = 47000]: 3.90479837638\n",
      "[ 0.5379059  -1.83325051 -6.92112948 -0.04939109 -0.32645705]\n",
      "\n",
      "Loss Before: 3.90479837638\n",
      "Loss After [i = 48000]: 3.90479837638\n",
      "[ 0.5379059  -1.83325057 -6.92112971 -0.04939109 -0.32645706]\n",
      "\n",
      "Loss Before: 3.90479837638\n",
      "Loss After [i = 49000]: 3.90479837638\n",
      "[ 0.5379059  -1.83325062 -6.92112988 -0.04939109 -0.32645706]\n",
      "Trained Loss:  3.90479837638\n",
      "Weights:  [ 0.5379059  -1.83325066 -6.92113    -0.04939109 -0.32645706]\n",
      "\n",
      "[ 2.8879039   1.83325066  6.92113   ]\n",
      "\n",
      "Line\n",
      "B: -0.417259016473\n",
      "XCoef: -0.264877362316\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAD8CAYAAABpXiE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHdJV+tqBPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a5439795f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "points, targets = generateChevronData()\n",
    "\n",
    "plt.axis([-1.5, 1.5, -1.5, 1.5])\n",
    "\n",
    "# Plot points on graph\n",
    "c1 = []\n",
    "c2 = []\n",
    "\n",
    "for i in range(0, len(points)):\n",
    "    if targets[i] == 0:\n",
    "        c1.append(points[i])\n",
    "    else:\n",
    "        c2.append(points[i])\n",
    "\n",
    "print(\"Type 0: \", len(c1))\n",
    "print(\"Type 1: \", len(c2))\n",
    "        \n",
    "plotScatter(c1)\n",
    "plotScatter(c2)\n",
    "\n",
    "weights = trainBoundaryHunter()\n",
    "\n",
    "# plt.scatter(weights[1], weights[2])\n",
    "plt.scatter(weights[3], weights[4])\n",
    "\n",
    "n = np.array([weights[0] + weights[1] * weights[3] + weights[2] * weights[4], \n",
    "              -weights[1], \n",
    "              -weights[2]])\n",
    "\n",
    "byas = -1 * n[0]/n[2]\n",
    "Xcoef = -1 * n[1]/n[2]\n",
    "\n",
    "print()\n",
    "print(n)\n",
    "print(\"\\nLine\")\n",
    "print(\"B: \" + str(byas))\n",
    "print(\"XCoef: \" + str(Xcoef))\n",
    "\n",
    "plt.plot([-1.0, 1.0], [-1*Xcoef + byas, Xcoef + byas], 'k-')\n",
    "plt.gca().set_aspect('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
