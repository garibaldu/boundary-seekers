\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\newtheorem{theorem}{Theorem}

\title{%
	Machine Learning: Collectives of local decision makers\\
	\large Construction Of A Boundary Hunter
}
\author{Daniel Braithwaite}

\begin{document}

\maketitle

The goal of this project is to create neurons which insted of searching for a global minumum they will choose a specific part of the data to place their hyperplane. Consider the folowing situation in $\mathbb{R}^2$ where we have data split into two classes and the second class is boxed into a chevron shape. A single perceptron will go and place a horizonal line through the plane however this isnt telling us anything interesting. Prehaps something better to do is to align its self to one of the sides of the chevron, then if we added another "boundary hunter" it could align its self to the other side of the chevron.

\section{Simplified Situation}
Before worrying about this in the general case we aim to get a better understanding of the problem by first trying to solve the case described above. We first make some changes to how we have paramaterised our perceptron. We want to find a way which peforms the same as a standard perceptron before we start making it into a boundary hunter

\subsection{Point \& Gradient Paramaterisation}
We are trying to optimise a point (x,y) and a gradient. This is an alternative way to paramaterise our line. We are using sum squared error for our loss function and sigmoid as our activation. Each "boundary hunter" (BH) has the folowing weights vector $W = [m, x_0, y_0]$ and we consider two inputs to our BH, x and y. Our perceptron computes $z = (y - y_0) - m(x - x_0)$ and then outputs $o = f(z) = \frac{1}{1+e^{-z}}$.

\subsubsection{Comparason To Perceptron}
If we use the same set of data to train both our standard perceptron and our modified perceptron we see that our modified dosnt quite peform as well as the standard version. For both models below they where trained for 50000 iterations of the training data.

\begin{figure}[H]
\centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Standard-Perceptron.png}
    \caption{Standard Perceptron (SSE = 4.90)}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Modified-Perceptron-(Point-Grad).png}
    \caption{Modified Perceptron (SSE = 7.56)}
  \end{minipage}
\end{figure}

I propose that this way of constructing our perceptron is less powerful than the standard way. Consider when we are optimising our point-slope representation, 

\begin{align*}
y - y_0 &= m(x - x_0) \\
y &= y_0 + mx - mx_0 \\
y &= mx + (y_0 + mx_0)
\end{align*}

We have just shown that (as we would expect) we can convert out point-slope representation into a intercept-slope representation. From here we see we can convert into something that could be represented by our origonal perceptron.

\begin{align*}
y = mx + (y_0 + mx_0)\\
\Rightarrow -(y_0 + mx_0) -mx + y
\end{align*}

So given the generic form of our standard perceptron $A + Bx + Cy$ we see here that our modified perceptron can only ever learn a representation where C = 1, limiting what we can achieve. An easy way to check this is to see what happens if we use a standard perceptron to learn the data but keep the third weight (i.e. C) fixed at 1. Sure enough we get the folowing result when we run that experement over 50000 iterations.

\begin{figure}[H]
\centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Standard-Perceptron-(C=1).png}
    \caption{Standard Perceptron with C = 1 (SSE = 7.56)}
  \end{minipage}
  \hfill
\end{figure}

So we are able to conclude that this method is unsuitable

\subsection{Normal \& Point Paramaterisation}
This method involves learning the normal vector (which is what the standard perceptron is doing) along with a point on the hyperplane. This increaes the number of paramaters we have to learn by n (when we are in $\mathbb{R}^n$ but this is necessary as to implement a BH we have to define some neighbourhood in which we care about and to do this we need to define it around some point on our hyperplane. So consider the folowing situation, we are learning the vector $n = [n_1, ..., n_n]$ normal to our hyperplane, the vector $a = [a_1, ..., a_n]$ which is on our hyperplane. We define $x = [x_1, ..., x_n]$ as our inputs making our weighted sum

\begin{align*}
z = \sum_{i=1}^n n_i * (a_i - x_i)
\end{align*}

We are still using SSE for loss and Sigmoid for actvation.

\subsubsection{Comparason To Perceptron}
Like before we compare the two paramaterisations using the same data and over 50000 iterations in the 2D case. In the graph for the modified perceptron the green point is our vector on the hyperplane.


\begin{figure}[H]
\centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Standard-Perceptron.png}
    \caption{Standard Perceptron (SSE = 3.90)}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Modified-Perceptron-(Normal-Point).png}
    \caption{Modified Perceptron (SSE = 3.90)}
  \end{minipage}
\end{figure}

This modified perceptron has all the quallitys needed to proceid with creating the boundary hunters, whats better is this also generalizes to the $\mathbb{R}^n$ case.

\section{Boundary Hunter}
Using the \textbf{Normal \& Point Paramaterisation} from before we construct boundary hunters, first we have a simple case where the area of intererest for each boundary hunter is simpley a circle with fixed radius, our goal is to make this area of interest an ellipse which the BH will learn.

\subsection{Loss Function Design: Attempt 1}
We wish to convert our Perceptron into a Boundary Hunter so to begin we start with designing a suitible loss function. So firstly we want to have some notion of how responsible a boundary hunter is for a given data point, for now we will just denote the responsibility for data point n by $r_n$ and worry about calculating this later. Consider our boundary hunter as reducing the uncertuanty of the classes for each of the data points. As the datapoints get further away from our radius of expertees we become more uncertain about the class of the datapoint. So a reasonable loss function is 

\begin{align}
\frac{1}{n} \sum_{i=0}^n r_i(C_i log(y_i) + (1-C_i)log(1-y_i)) + (1-r_i) log(1/2)
\end{align}


\subsection{Responsibility Function Design}
How do we construct $r_i$. We would like the folowing propertys of a responsibility function

\begin{enumerate}
\item Making the area of interest include all points will reduce loss to standard log loss.
\item All points inside the area of interest should have the same responsibility.
\item Responsibility for points outside the area of interest should decrease as distance from area increases.
\end{enumerate}

The folowing function seems like a good place to start. The responsiblity for any point inside the area of interest is 1, and as the data points get further away the responsiblity decreases

\begin{align}
r_i = 1 - \frac{max(0, d-r)}{abs(d-r) + 0.1}
\end{align}

However we note that this functions isnt smooth, and that can cause problems when training with back prop, not to mention the hard boundary (i.e. responsibility 1 for all points inside area of interest) will also have a negative effect on the gradients. While its reasonable for us to want property (2) it seems unlikely we will find a sutible function meeting the condition. So we remove property (2) and add the condition that our function must be smooth. We now turn to the logistic function, Let $f = 1 - \frac{1}{1 + e^{-S(d - r)}}$, where S is the steepness of the sigmoid. We will want our responsibility function to have a low sensitivity at each extreme, i.e. a small change in distance when the point is either very close to or far from our center point should have a neglible inpact on the responsiblity. How ever we want our function to be very sensitive around the border of our area of interest. By a purely arbatary decision and from inspecting the graphs I chose to have a steppness of 10 thus giving us the folowing function

\begin{align}
r_i = 1 - \frac{1}{1 + e^{-10(x-r)}}
\end{align}

\begin{figure}[H]
\centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Responsibility-Function.jpg}
    \caption{Graph of responsibility function with r = 0.1, 0.3, 0.5, 0.7, 0.9}
  \end{minipage}
  \hfill
\end{figure}

While this does not meet our origonal criteria we see that for most points inside our area of interest the responsibility will be "close" to 1 and certianly above $\frac{1}{2}$, And if the area includes all the points then our loss will not reduce to the standard log loss but it will be close to it and as $r \to\infty$ we will converge to the standard log loss.

\subsubsection{Experemental Results}
We train our boundary hunter over the same data for 10000 iterations and the result is somewhat uninteresting, it simply learns an area of interest which contains no points in the data set

\begin{figure}[H]
\centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{BoundaryHunter-Attempt1-01.png}
    \caption{Boundary Hunter with (1) as loss and (3) as responsibiliy}
  \end{minipage}
  \hfill
\end{figure}

As demonstraited using this loss does not produce any significant results.

\subsubsection{Justification For Results}
So consider our loss for a given data point with target $t$, prediction $y$ and responsibility $r$. WILOG assume $t = 1$. So then we are left with $r\ log(y) + (1-r)\ log(\frac{1}{2})$. We know that $log(y) \leq log(\frac{1}{2})$

\subsubsection{Reflection}
Esentially we want to reward our boundary hunter if it is an expert in a given region, so if it gets things wrong outside its area of interest thats good, and if it gets things correct outside its area of interest then thats bad.

\subsection{Loss Funcction Design: Attempt 2}
Based on our previous findings a more in depth consideration of a suitable loss function is required. Consider the folowing properties of our ideal loss function. The basic idea is that we want to become experts in a given region
\begin{enumerate}
\item As a data point gets further away from our area of interest we "care less" about our accuracy for classifying that point.
\item Increasing area of interest should decrease the loss iff increasing the area of interest allows us to classify more points correctly.
\item Decreasing area of interest should decrease the loss iff reducing area of interest allows us to classify less points incorrectly in our area
\end{enumerate}

We can simplyfy these ideas into rules

\begin{enumerate}
\item We want to be as accurate as possible in our area of interest and care less about our accuracy based on how "responsible" we are for the data point.
\item We should be penalised for classifying things correctly out side our area of interest based on how "not responsible" we are for the data point (i.e. if we are classifying a data point correctly but it is just outside our area of interest then we should be penalised).
\item We should be rewarded for classifying things incorrectly our side our area of interest based on how "not responsible" we are for the data point (i.e. if we are classifying a data point incorrectly and its just outside our area of interest then we should be rewarded).
\end{enumerate}

We now wish to convert this into a loss function. Item (1) is refering to $r_n$ multiplyed by the standard cross entropy, restricted to points inside our area of interest. Then items (2) + (3) are refering to the oposite of our cross entropy multiplyed by $r_n$ restricted to all points outside our area of interest.

\begin{align}
- \big( \frac{1}{|I|} \big[ \sum_{i \in I}^n H(t_i, y_i) \big] + \frac{1}{|O|} \big[ \sum_{i \in O} H(\not t_i, y_i) \big] \big)
\end{align}

Where $I$ is the set of points inside our area and $O$ is the set of points not in our area of interest.

\subsubsection{Experemental Results}

With the same paramaters as before we train a perceptron with this loss and find equally uninteresting results. Esentailly the boundary hunter ends up positioning not doing anything useful at all

\begin{figure}[H]
\centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{BoundaryHunter-Attempt2-01.png}
    \caption{Boundary Hunter with (?) as loss and (3) as responsibiliy}
  \end{minipage}
  \hfill
\end{figure}

\subsubsection{Reflection}
The conditions used to construct this loss are prehaps to harsh, currently we are trying to maximise our error outside while minimizing our error inside. Where we dont actually care how incorrect we are outside our area.

\subsection{Loss Function Design: Important Observation}
We take a step back and consider the basics of what we are trying to achieve. First we make an interesting observation which will change the way we approach designing this loss. Consider the simple loss function $L =\sum R(r) * CE$ where $R(r)$ is our responsibility function with radius as paramater and $CE$ is our cross entropy. And we want to compute the quantity $\frac{\partial r}{\partial L}$ so we can train r. We observe the folowing

\begin{align*}
\frac{\partial L}{\partial r} &= \frac{\partial}{\partial r} \big[ \sum R(r) * CE \big] \\
&= \sum R^{'} (r) * CE
\end{align*} 

Namely the quantity $\frac{\partial L}{\partial r}$ dosnt actually tell us how changing the radius changes the error it simplys tells us which direction to move the radius to increase the responsitility. This results in the radius constantly increasing.\\

\subsection{Loss Function Design: Attempt 3}
Based on what we just observed we need a new approach. We consider our boundary hunter as a sales man, who gets paied $\$B$ for selling something to someone who wants it (if the sales man is respoinbile for this individual) and gets penalised $\$C$ dollars for selling something to someone who dosnt want it. Using this model, the sales man's is to position them selves and adjust there responsibility so that they are maximising there proffit. Now we simply must quantify this. We know $y^t(1-y)^{t-1}$ is the probability we get something right (probability we are selling to someone who wants what we are selling). We arrive at the folowing quantity.

\begin{align}
L = \sum_{i=0} r_i * \big[ By_i^{t_i}(1-y_i)^{1-t_i} - Cy_i^{1-t_i}(1-y_i)^{t_i} \big] 
\end{align}

While intially it seemes like this would suffer from the same problem described in the previous observation it does not. Consider the differential we know that $R^{'}(r)$ will always have the same sign but $By_i^{t_i}(1-y_i)^{1-t_i} - Cy_i^{1-t_i}(1-y_i)^{t_i}$ can be both positive and negative, so we end up in the situation where if we are lossing more money that we are gaining then we will decreases our radius but if we are gaining more money than we are lossing we will increase our radius

\subsubsection{Experemental Results}

With the same paramaters as before we train a perceptron with this loss and find that it grows to fit all the data

\begin{figure}[H]
\centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{BoundaryHunter-Attempt3-R2.png}
    \caption{Boundary Hunter with (?) as loss and (3) as responsibiliy}
  \end{minipage}
  \hfill
\end{figure}

\subsubsection{Loss Investigation}
To get a better idea of peformance we will execute a boundary hunter with this loss over various random initial conditions to get a feel for how it works, the results are somewhat dissapointing however

\begin{figure}[H]
\centering
  \subfloat[]{\includegraphics[width=0.3\textwidth]{BoundaryHunter-Attempt3-R0.png}}
  \hfill
  \subfloat[]{\includegraphics[width=0.3\textwidth]{BoundaryHunter-Attempt3-R1.png}}
\hfill
\subfloat[]{\includegraphics[width=0.3\textwidth]{BoundaryHunter-Attempt3-R2.png}}
\end{figure}

This approach definitly seemes to favour moving off to the side of the data. While (c) seems like a reasonable solution and actually contains some points in its area of interest

\subsubsection{Justification For Results}
Using the responsibility to scale the loss in this way encourages the boundary hunter to "run away" from the data because anything outside its area of interest will have a significantly lower responsibility than anything inside, So even by getting everything right inside our area we can achieve a better loss by moving away from the data and things we might be getting wrong outside our area.

\subsubsection{Reflection}
While we need to use the responsibility in some reguard, using it directly in the loss function does not provide the results we are looking for.


\subsection{Loss Function Design: Attempt 4}
Continuing with the same analgey as before (the sales man trying to maximise there profit). Except now we only sell to people inside our area of interest and anyone outside we dont care about. This will solve the problem of boundary hunter "escaping" the data. If we are earning lots of money from selling inside our area then we wish to expand the region we are selling to and then if we are loosing lots of money we want to reduce the region we are selling to. This corosponds to the same loss as Attempt 3 however now we are restircting our loss to only care about points inside our area. This way we are optimising our hyperplane only for the points we care about.

However we have to define the gradient of the radius as it nolonger exsists, we set it to be $\frac{\partial L}{\partial r} = sgn(loss)$  so we get -1 if the loss is negative, 0 if the loss is, and 1 if the loss is positive

\subsubsection{Experemental Results}
In our example data there are many more of one class than the other and using this method rewards the boundary hunter for just classifying everything as the one class and including everything inside our range.

\begin{figure}[H]
\centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{BoundaryHunter-Attempt4-01.png}
    \caption{Boundary Hunter with (?) as loss and (3) as responsibiliy}
  \end{minipage}
  \hfill
\end{figure}

However our loss gives us a knob which we can turn, changing how much we care about getting things wrong or getting them right, we can adjust this knob so that we hate getting things wrong more. We set our cost multiplyer to be $1 + (\frac{65}{100})$, and profit multipler to  $1 + (\frac{35}{100})$. The aim of this is to balance out the proportions of points.

\begin{figure}[H]
\centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{BoundaryHunter-Attempt4-02.png}
    \caption{}
  \end{minipage}
  \hfill
\end{figure}
While this looks like ezactly what we want, this is just a coincidence! Now we encouter an issue with the loss reaching 0 and then staying there, esentially bringing training to a halt, to fix this we count a loss of 0 as making us want to increase our region. While this does get things moving the boundary hunters like to keep there regions small. Experementing with this implementation, starting with random initial conditions will allow us to get a better feel for wether this is working or not.

\begin{figure}[H]
\centering
  \subfloat[]{\includegraphics[width=0.4\textwidth]{BoundaryHunter-Attempt4-03.png}}
  \hfill
  \subfloat[]{\includegraphics[width=0.4\textwidth]{BoundaryHunter-Attempt4-04.png}}
\end{figure}

While these results still seem interesting and the approach seemes promising but radiuses are to small to produce any meaningful results. How can we improve this loss to consider larger regions.

\end{document}
