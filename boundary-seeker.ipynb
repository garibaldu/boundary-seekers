{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy.random as rng\n",
    "import pylab as pl\n",
    "import autograd.numpy as np\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Npats, Nins = 60, 2\n",
    "X = 1*rng.normal(1,1,size=(Npats,Nins))\n",
    "w_truth = rng.normal(0,1,size=(Nins))\n",
    "m_truth = rng.normal(1,1,size=(Nins))\n",
    "phi = np.dot(X - m_truth, w_truth)\n",
    "Targ = np.where(phi > 0.0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss under a Local Perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(phi):\n",
    "    return 1.0/(1.0 + np.exp(-phi))\n",
    "\n",
    "def calc_prob_class1(params):\n",
    "    # Sigmoid perceptron ('logistic regression')\n",
    "    tildex = X - params['mean']\n",
    "    W = params['wgts']\n",
    "    phi = np.dot(tildex, W)\n",
    "    return sigmoid(phi)  # Sigmoid perceptron ('logistic regression')\n",
    "\n",
    "def calc_membership(params):\n",
    "    # NB. this is just a helper function for training_loss really.\n",
    "    tildex = X - params['mean']\n",
    "    W, r2, R2 = params['wgts'], params['r2'], params['R2']\n",
    "    Dr2 = np.power(np.dot(tildex, W), 2.0)\n",
    "    L2X = (np.power(tildex, 2.0)).sum(1)\n",
    "    DR2 = L2X - Dr2\n",
    "    dist2 = (Dr2/r2) + (DR2/R2)  # rescaled 'distance' to the shifted 'origin'\n",
    "    membership = np.exp(-0.5*dist2)\n",
    "    #print(membership)\n",
    "    return np.array(membership)\n",
    "\n",
    "\n",
    "\n",
    "def classification_loss(params):\n",
    "    membership = calc_membership(params)\n",
    "    Y = calc_prob_class1(params)\n",
    "    return np.sum(membership*(Targ*np.log(Y) + (1-Targ)*np.log(1-Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss under a Mixture of Gaussians model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MoG_loss(params):\n",
    "    membership = calc_membership(params)\n",
    "    return np.sum(membership)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use autograd for functions that deliver gradients of those losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classification_gradient = grad(classification_loss)\n",
    "MoG_gradient = grad(MoG_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just a pretty display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red and Black are target 0 and 1 patterns respectively.\n",
    "\n",
    "They will get \"filled in\" once the perceptron is getting them correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Be able to show the current solution, against the data in 2D.\n",
    "def show_result(params, X, Targ):\n",
    "    print(\"Parameters:\")\n",
    "    for key in params.keys():\n",
    "        print(key,'\\t', params[key])\n",
    "    print(\"Loss:\", training_loss(params))\n",
    "    membership = calc_membership(params)\n",
    "    Y = calc_prob_class1(params)\n",
    "\n",
    "    pl.clf()\n",
    "    marksize = 8\n",
    "    cl ={0:'red', 1:'black'}\n",
    "    for i, x in enumerate(X):\n",
    "        pl.plot(x[0],x[1],'x',color=cl[int(Targ[i])],alpha=.4,markersize=marksize)\n",
    "        pl.plot(x[0],x[1],'o',color=cl[int(Targ[i])],alpha=1.-float(abs(Targ[i]-Y[i])),markersize=marksize)\n",
    "        \n",
    "    pl.axis('equal')\n",
    "    s = X.ravel().max() - X.ravel().min()\n",
    "    m, w = params['mean'], params['wgts']\n",
    "    # Show the mean in blue\n",
    "    #pl.arrow(0, 0, m[0], m[1], head_width=0.25, head_length=0.5, fc='b', ec='b', linewidth=1, alpha=.95)\n",
    "    # Show the perceptron decision boundary, in green\n",
    "    pl.arrow(m[0]-w[0], m[1]-w[1], w[0], w[1], head_width=s, head_length=s/5, fc='g', ec='g', linewidth=3, alpha=.5)\n",
    "    pl.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning, starting from random weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_one_learning_step(params,X,Targ,rate):\n",
    "    grads = classification_gradient(params)\n",
    "    params['wgts'] = params['wgts'] + rate * grads['wgts']   # one step of learning\n",
    "    params['mean'] = params['mean'] + rate * grads['mean']   # one step of learning\n",
    "    return (params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_w = rng.normal(0,1,size=(Nins))\n",
    "init_m = 4.*rng.normal(0,1,size=(Nins))\n",
    "rate = 0.5 / Npats\n",
    "params = {'wgts':init_w, 'mean':init_m, 'r2':1000.0, 'R2':1000.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "wgts \t [-3.68363354 -3.99124982]\n",
      "mean \t [ 2.80316343 -0.10518492]\n",
      "r2 \t 1000.0\n",
      "R2 \t 1000.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'training_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b6c4aacfd4fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_one_learning_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTarg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mshow_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'wgts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-49fc86e53cc0>\u001b[0m in \u001b[0;36mshow_result\u001b[1;34m(params, X, Targ)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mmembership\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_membership\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_prob_class1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_loss' is not defined"
     ]
    }
   ],
   "source": [
    "for t in range(250):\n",
    "    params = do_one_learning_step(params,X,Targ,rate)\n",
    "\n",
    "show_result(params, X, Targ)\n",
    "\n",
    "Y = sigmoid(np.dot(X-params['mean'], params['wgts'])) \n",
    "print('vanilla loss: ', np.sum(Targ*np.log(Y) + (1-Targ)*np.log(1-Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Mixture of Gaussians knob?\n",
    "\n",
    "My model above used the loss function\n",
    "$$ \\mathcal{L} = \\sum_n \\rho_{n} \\;\\, \\log P(\\mathbf{x}_n \\; \\text{classified correctly}) $$\n",
    "where $\\rho_n$ is the \"membership\". I set $\\rho_n = \\exp(-d^2_n/2)$\n",
    "where  $d_{n}$ is the Mahalanobis distance of the n-th input $\\mathbf{x}_n$ under the current Gaussian.\n",
    "\n",
    "Suppose this actually works and we have a bunch of such \"boundary hunters\".\n",
    "\n",
    "They're each capable of independently doing something sensible, unlike vanilla perceptrons. But we do want them to interact just enough to stay out of each others' way. This makes me wonder whether using MoG to initialise them might be a good idea, and THAT makes me wonder whether we can just interpolate smoothly between these two loss functions. ie. is there a \"knob\" we can turn, which when set at one extreme gives the MoG and at the other gives a set of class boundaries? It would probably be a cute thing to be able to do, basically.\n",
    "\n",
    "### MoG Loss function\n",
    "The loss of a MoG model is a sum over training items, which are indexed by $n$.\n",
    "\n",
    "For simplicity, let's suppose the $K$ mixture components all have the same prior $\\pi_k = 1/K$, and same \"volume\" (determinant of the inverse of the covariance matrix, or somesuch). \n",
    "\n",
    "Then the probability density $\\rho_{nk} = P(\\mathbf{x}_n | k) \\propto \\exp(-d_{nk}^2 / 2)$. \n",
    "\n",
    "We can denote a \"responsibility\" (posterior probability) of the k-th Gaussian for the n-th input pattern \n",
    "$$ r_{nk} = \\frac{\\rho_{nk}}{\\displaystyle{ \\sum_{k^\\prime} \\rho_{nk^\\prime}}}$$.\n",
    "\n",
    "\n",
    "Per item, the loss of a MoG model is then\n",
    "$$ \\mathcal{L}_\\text{MoG} = \\sum_k r_{nk} \\;\\, \\log P(\\mathbf{x}_n \\; \\text{generated} | k) $$\n",
    "\n",
    "\n",
    "I suspect a plausible loss function to use for the \"hoarde of boundary hunters\" could be (per item) as follows, which is very very super similar:\n",
    "$$ \\mathcal{L}_\\text{HoBH} = \\sum_k r_{nk} \\;\\, \\log P(\\mathbf{x}_n \\; \\text{classified correctly} | k) $$\n",
    "\n",
    "If so, that's quite groovy as a generative model: the universe has to (a) place the point correctly, and (b) give it the right class to nail the whole problem.\n",
    "ie. we could have a combined model with just uses the sum of the two losses, say, to model both the density and the class information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
